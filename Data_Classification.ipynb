{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tokaalaa/DM_Course/blob/main/Data_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4xY5eZ-k84n"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVj727bGlB8a"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo64pOqlmVlV",
        "outputId": "15ff8a6e-cc1c-43d2-acfe-d8338a9b0044"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtBMXe6slBFr"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwV6zpHIlgsP",
        "outputId": "e8ff90aa-ebf6-4204-efdc-2084204fed0a"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/Assignment_3_DM/Data/wdbc.data' \r\n",
        "names_path = '/content/drive/MyDrive/Assignment_3_DM/Data/wdbc.names'\r\n",
        "with open(names_path, 'r') as f:\r\n",
        "    names = f.read()\r\n",
        "print(names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Title: Wisconsin Diagnostic Breast Cancer (WDBC)\n",
            "\n",
            "2. Source Information\n",
            "\n",
            "a) Creators: \n",
            "\n",
            "\tDr. William H. Wolberg, General Surgery Dept., University of\n",
            "\tWisconsin,  Clinical Sciences Center, Madison, WI 53792\n",
            "\twolberg@eagle.surgery.wisc.edu\n",
            "\n",
            "\tW. Nick Street, Computer Sciences Dept., University of\n",
            "\tWisconsin, 1210 West Dayton St., Madison, WI 53706\n",
            "\tstreet@cs.wisc.edu  608-262-6619\n",
            "\n",
            "\tOlvi L. Mangasarian, Computer Sciences Dept., University of\n",
            "\tWisconsin, 1210 West Dayton St., Madison, WI 53706\n",
            "\tolvi@cs.wisc.edu \n",
            "\n",
            "b) Donor: Nick Street\n",
            "\n",
            "c) Date: November 1995\n",
            "\n",
            "3. Past Usage:\n",
            "\n",
            "first usage:\n",
            "\n",
            "\tW.N. Street, W.H. Wolberg and O.L. Mangasarian \n",
            "\tNuclear feature extraction for breast tumor diagnosis.\n",
            "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
            "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
            "\n",
            "OR literature:\n",
            "\n",
            "\tO.L. Mangasarian, W.N. Street and W.H. Wolberg. \n",
            "\tBreast cancer diagnosis and prognosis via linear programming. \n",
            "\tOperations Research, 43(4), pages 570-577, July-August 1995.\n",
            "\n",
            "Medical literature:\n",
            "\n",
            "\tW.H. Wolberg, W.N. Street, and O.L. Mangasarian. \n",
            "\tMachine learning techniques to diagnose breast cancer from\n",
            "\tfine-needle aspirates.  \n",
            "\tCancer Letters 77 (1994) 163-171.\n",
            "\n",
            "\tW.H. Wolberg, W.N. Street, and O.L. Mangasarian. \n",
            "\tImage analysis and machine learning applied to breast cancer\n",
            "\tdiagnosis and prognosis.  \n",
            "\tAnalytical and Quantitative Cytology and Histology, Vol. 17\n",
            "\tNo. 2, pages 77-87, April 1995. \n",
            "\n",
            "\tW.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. \n",
            "\tComputerized breast cancer diagnosis and prognosis from fine\n",
            "\tneedle aspirates.  \n",
            "\tArchives of Surgery 1995;130:511-516.\n",
            "\n",
            "\tW.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. \n",
            "\tComputer-derived nuclear features distinguish malignant from\n",
            "\tbenign breast cytology.  \n",
            "\tHuman Pathology, 26:792--796, 1995.\n",
            "\n",
            "See also:\n",
            "\thttp://www.cs.wisc.edu/~olvi/uwmp/mpml.html\n",
            "\thttp://www.cs.wisc.edu/~olvi/uwmp/cancer.html\n",
            "\n",
            "Results:\n",
            "\n",
            "\t- predicting field 2, diagnosis: B = benign, M = malignant\n",
            "\t- sets are linearly separable using all 30 input features\n",
            "\t- best predictive accuracy obtained using one separating plane\n",
            "\t\tin the 3-D space of Worst Area, Worst Smoothness and\n",
            "\t\tMean Texture.  Estimated accuracy 97.5% using repeated\n",
            "\t\t10-fold crossvalidations.  Classifier has correctly\n",
            "\t\tdiagnosed 176 consecutive new patients as of November\n",
            "\t\t1995. \n",
            "\n",
            "4. Relevant information\n",
            "\n",
            "\tFeatures are computed from a digitized image of a fine needle\n",
            "\taspirate (FNA) of a breast mass.  They describe\n",
            "\tcharacteristics of the cell nuclei present in the image.\n",
            "\tA few of the images can be found at\n",
            "\thttp://www.cs.wisc.edu/~street/images/\n",
            "\n",
            "\tSeparating plane described above was obtained using\n",
            "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
            "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
            "\tpp. 97-101, 1992], a classification method which uses linear\n",
            "\tprogramming to construct a decision tree.  Relevant features\n",
            "\twere selected using an exhaustive search in the space of 1-4\n",
            "\tfeatures and 1-3 separating planes.\n",
            "\n",
            "\tThe actual linear program used to obtain the separating plane\n",
            "\tin the 3-dimensional space is that described in:\n",
            "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
            "\tOptimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "\n",
            "\tThis database is also available through the UW CS ftp server:\n",
            "\n",
            "\tftp ftp.cs.wisc.edu\n",
            "\tcd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            "5. Number of instances: 569 \n",
            "\n",
            "6. Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)\n",
            "\n",
            "7. Attribute information\n",
            "\n",
            "1) ID number\n",
            "2) Diagnosis (M = malignant, B = benign)\n",
            "3-32)\n",
            "\n",
            "Ten real-valued features are computed for each cell nucleus:\n",
            "\n",
            "\ta) radius (mean of distances from center to points on the perimeter)\n",
            "\tb) texture (standard deviation of gray-scale values)\n",
            "\tc) perimeter\n",
            "\td) area\n",
            "\te) smoothness (local variation in radius lengths)\n",
            "\tf) compactness (perimeter^2 / area - 1.0)\n",
            "\tg) concavity (severity of concave portions of the contour)\n",
            "\th) concave points (number of concave portions of the contour)\n",
            "\ti) symmetry \n",
            "\tj) fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "Several of the papers listed above contain detailed descriptions of\n",
            "how these features are computed. \n",
            "\n",
            "The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "largest values) of these features were computed for each image,\n",
            "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
            "13 is Radius SE, field 23 is Worst Radius.\n",
            "\n",
            "All feature values are recoded with four significant digits.\n",
            "\n",
            "8. Missing attribute values: none\n",
            "\n",
            "9. Class distribution: 357 benign, 212 malignant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "jIytLuQZlh3E",
        "outputId": "ed38a9dd-b869-4b70-ea09-be995586d177"
      },
      "source": [
        "attr_names = ['Id', 'diagnosis']\r\n",
        "for i in range (1,31):\r\n",
        "    attr_names.append(\"attr \"+ str(i))\r\n",
        "\r\n",
        "data = pd.read_csv(data_path, sep=',',names=attr_names)\r\n",
        "print(data.shape)\r\n",
        "data.head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>attr 1</th>\n",
              "      <th>attr 2</th>\n",
              "      <th>attr 3</th>\n",
              "      <th>attr 4</th>\n",
              "      <th>attr 5</th>\n",
              "      <th>attr 6</th>\n",
              "      <th>attr 7</th>\n",
              "      <th>attr 8</th>\n",
              "      <th>attr 9</th>\n",
              "      <th>attr 10</th>\n",
              "      <th>attr 11</th>\n",
              "      <th>attr 12</th>\n",
              "      <th>attr 13</th>\n",
              "      <th>attr 14</th>\n",
              "      <th>attr 15</th>\n",
              "      <th>attr 16</th>\n",
              "      <th>attr 17</th>\n",
              "      <th>attr 18</th>\n",
              "      <th>attr 19</th>\n",
              "      <th>attr 20</th>\n",
              "      <th>attr 21</th>\n",
              "      <th>attr 22</th>\n",
              "      <th>attr 23</th>\n",
              "      <th>attr 24</th>\n",
              "      <th>attr 25</th>\n",
              "      <th>attr 26</th>\n",
              "      <th>attr 27</th>\n",
              "      <th>attr 28</th>\n",
              "      <th>attr 29</th>\n",
              "      <th>attr 30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.990</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.049040</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.015870</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.71190</td>\n",
              "      <td>0.26540</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.570</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.24160</td>\n",
              "      <td>0.18600</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.690</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.040060</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.020580</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.45040</td>\n",
              "      <td>0.24300</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.420</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.074580</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.018670</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.68690</td>\n",
              "      <td>0.25750</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.290</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.018850</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.40000</td>\n",
              "      <td>0.16250</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.450</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.15780</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>0.2087</td>\n",
              "      <td>0.07613</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.8902</td>\n",
              "      <td>2.217</td>\n",
              "      <td>27.19</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.033450</td>\n",
              "      <td>0.03672</td>\n",
              "      <td>0.011370</td>\n",
              "      <td>0.02165</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.53550</td>\n",
              "      <td>0.17410</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>844359</td>\n",
              "      <td>M</td>\n",
              "      <td>18.250</td>\n",
              "      <td>19.98</td>\n",
              "      <td>119.60</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>0.09463</td>\n",
              "      <td>0.10900</td>\n",
              "      <td>0.11270</td>\n",
              "      <td>0.07400</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.05742</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>3.180</td>\n",
              "      <td>53.91</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.013820</td>\n",
              "      <td>0.02254</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.01369</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>22.88</td>\n",
              "      <td>27.66</td>\n",
              "      <td>153.20</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.37840</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.08368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>84458202</td>\n",
              "      <td>M</td>\n",
              "      <td>13.710</td>\n",
              "      <td>20.83</td>\n",
              "      <td>90.20</td>\n",
              "      <td>577.9</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.16450</td>\n",
              "      <td>0.09366</td>\n",
              "      <td>0.05985</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07451</td>\n",
              "      <td>0.5835</td>\n",
              "      <td>1.3770</td>\n",
              "      <td>3.856</td>\n",
              "      <td>50.96</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.030290</td>\n",
              "      <td>0.02488</td>\n",
              "      <td>0.014480</td>\n",
              "      <td>0.01486</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>17.06</td>\n",
              "      <td>28.14</td>\n",
              "      <td>110.60</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.1654</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.26780</td>\n",
              "      <td>0.15560</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.11510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>844981</td>\n",
              "      <td>M</td>\n",
              "      <td>13.000</td>\n",
              "      <td>21.82</td>\n",
              "      <td>87.50</td>\n",
              "      <td>519.8</td>\n",
              "      <td>0.12730</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.18590</td>\n",
              "      <td>0.09353</td>\n",
              "      <td>0.2350</td>\n",
              "      <td>0.07389</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>1.0020</td>\n",
              "      <td>2.406</td>\n",
              "      <td>24.32</td>\n",
              "      <td>0.005731</td>\n",
              "      <td>0.035020</td>\n",
              "      <td>0.03553</td>\n",
              "      <td>0.012260</td>\n",
              "      <td>0.02143</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>15.49</td>\n",
              "      <td>30.73</td>\n",
              "      <td>106.20</td>\n",
              "      <td>739.3</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.5401</td>\n",
              "      <td>0.53900</td>\n",
              "      <td>0.20600</td>\n",
              "      <td>0.4378</td>\n",
              "      <td>0.10720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>84501001</td>\n",
              "      <td>M</td>\n",
              "      <td>12.460</td>\n",
              "      <td>24.04</td>\n",
              "      <td>83.97</td>\n",
              "      <td>475.9</td>\n",
              "      <td>0.11860</td>\n",
              "      <td>0.23960</td>\n",
              "      <td>0.22730</td>\n",
              "      <td>0.08543</td>\n",
              "      <td>0.2030</td>\n",
              "      <td>0.08243</td>\n",
              "      <td>0.2976</td>\n",
              "      <td>1.5990</td>\n",
              "      <td>2.039</td>\n",
              "      <td>23.94</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.072170</td>\n",
              "      <td>0.07743</td>\n",
              "      <td>0.014320</td>\n",
              "      <td>0.01789</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>15.09</td>\n",
              "      <td>40.68</td>\n",
              "      <td>97.65</td>\n",
              "      <td>711.4</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>1.0580</td>\n",
              "      <td>1.10500</td>\n",
              "      <td>0.22100</td>\n",
              "      <td>0.4366</td>\n",
              "      <td>0.20750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>845636</td>\n",
              "      <td>M</td>\n",
              "      <td>16.020</td>\n",
              "      <td>23.24</td>\n",
              "      <td>102.70</td>\n",
              "      <td>797.8</td>\n",
              "      <td>0.08206</td>\n",
              "      <td>0.06669</td>\n",
              "      <td>0.03299</td>\n",
              "      <td>0.03323</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.05697</td>\n",
              "      <td>0.3795</td>\n",
              "      <td>1.1870</td>\n",
              "      <td>2.466</td>\n",
              "      <td>40.51</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>0.009269</td>\n",
              "      <td>0.01101</td>\n",
              "      <td>0.007591</td>\n",
              "      <td>0.01460</td>\n",
              "      <td>0.003042</td>\n",
              "      <td>19.19</td>\n",
              "      <td>33.88</td>\n",
              "      <td>123.80</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>0.1181</td>\n",
              "      <td>0.1551</td>\n",
              "      <td>0.14590</td>\n",
              "      <td>0.09975</td>\n",
              "      <td>0.2948</td>\n",
              "      <td>0.08452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>84610002</td>\n",
              "      <td>M</td>\n",
              "      <td>15.780</td>\n",
              "      <td>17.89</td>\n",
              "      <td>103.60</td>\n",
              "      <td>781.0</td>\n",
              "      <td>0.09710</td>\n",
              "      <td>0.12920</td>\n",
              "      <td>0.09954</td>\n",
              "      <td>0.06606</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.06082</td>\n",
              "      <td>0.5058</td>\n",
              "      <td>0.9849</td>\n",
              "      <td>3.564</td>\n",
              "      <td>54.16</td>\n",
              "      <td>0.005771</td>\n",
              "      <td>0.040610</td>\n",
              "      <td>0.02791</td>\n",
              "      <td>0.012820</td>\n",
              "      <td>0.02008</td>\n",
              "      <td>0.004144</td>\n",
              "      <td>20.42</td>\n",
              "      <td>27.28</td>\n",
              "      <td>136.50</td>\n",
              "      <td>1299.0</td>\n",
              "      <td>0.1396</td>\n",
              "      <td>0.5609</td>\n",
              "      <td>0.39650</td>\n",
              "      <td>0.18100</td>\n",
              "      <td>0.3792</td>\n",
              "      <td>0.10480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>846226</td>\n",
              "      <td>M</td>\n",
              "      <td>19.170</td>\n",
              "      <td>24.80</td>\n",
              "      <td>132.40</td>\n",
              "      <td>1123.0</td>\n",
              "      <td>0.09740</td>\n",
              "      <td>0.24580</td>\n",
              "      <td>0.20650</td>\n",
              "      <td>0.11180</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07800</td>\n",
              "      <td>0.9555</td>\n",
              "      <td>3.5680</td>\n",
              "      <td>11.070</td>\n",
              "      <td>116.20</td>\n",
              "      <td>0.003139</td>\n",
              "      <td>0.082970</td>\n",
              "      <td>0.08890</td>\n",
              "      <td>0.040900</td>\n",
              "      <td>0.04484</td>\n",
              "      <td>0.012840</td>\n",
              "      <td>20.96</td>\n",
              "      <td>29.94</td>\n",
              "      <td>151.70</td>\n",
              "      <td>1332.0</td>\n",
              "      <td>0.1037</td>\n",
              "      <td>0.3903</td>\n",
              "      <td>0.36390</td>\n",
              "      <td>0.17670</td>\n",
              "      <td>0.3176</td>\n",
              "      <td>0.10230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>846381</td>\n",
              "      <td>M</td>\n",
              "      <td>15.850</td>\n",
              "      <td>23.95</td>\n",
              "      <td>103.70</td>\n",
              "      <td>782.7</td>\n",
              "      <td>0.08401</td>\n",
              "      <td>0.10020</td>\n",
              "      <td>0.09938</td>\n",
              "      <td>0.05364</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.05338</td>\n",
              "      <td>0.4033</td>\n",
              "      <td>1.0780</td>\n",
              "      <td>2.903</td>\n",
              "      <td>36.58</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>0.031260</td>\n",
              "      <td>0.05051</td>\n",
              "      <td>0.019920</td>\n",
              "      <td>0.02981</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>16.84</td>\n",
              "      <td>27.66</td>\n",
              "      <td>112.00</td>\n",
              "      <td>876.5</td>\n",
              "      <td>0.1131</td>\n",
              "      <td>0.1924</td>\n",
              "      <td>0.23220</td>\n",
              "      <td>0.11190</td>\n",
              "      <td>0.2809</td>\n",
              "      <td>0.06287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>84667401</td>\n",
              "      <td>M</td>\n",
              "      <td>13.730</td>\n",
              "      <td>22.61</td>\n",
              "      <td>93.60</td>\n",
              "      <td>578.3</td>\n",
              "      <td>0.11310</td>\n",
              "      <td>0.22930</td>\n",
              "      <td>0.21280</td>\n",
              "      <td>0.08025</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.07682</td>\n",
              "      <td>0.2121</td>\n",
              "      <td>1.1690</td>\n",
              "      <td>2.061</td>\n",
              "      <td>19.21</td>\n",
              "      <td>0.006429</td>\n",
              "      <td>0.059360</td>\n",
              "      <td>0.05501</td>\n",
              "      <td>0.016280</td>\n",
              "      <td>0.01961</td>\n",
              "      <td>0.008093</td>\n",
              "      <td>15.03</td>\n",
              "      <td>32.01</td>\n",
              "      <td>108.80</td>\n",
              "      <td>697.7</td>\n",
              "      <td>0.1651</td>\n",
              "      <td>0.7725</td>\n",
              "      <td>0.69430</td>\n",
              "      <td>0.22080</td>\n",
              "      <td>0.3596</td>\n",
              "      <td>0.14310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>84799002</td>\n",
              "      <td>M</td>\n",
              "      <td>14.540</td>\n",
              "      <td>27.54</td>\n",
              "      <td>96.73</td>\n",
              "      <td>658.8</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.15950</td>\n",
              "      <td>0.16390</td>\n",
              "      <td>0.07364</td>\n",
              "      <td>0.2303</td>\n",
              "      <td>0.07077</td>\n",
              "      <td>0.3700</td>\n",
              "      <td>1.0330</td>\n",
              "      <td>2.879</td>\n",
              "      <td>32.55</td>\n",
              "      <td>0.005607</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.010900</td>\n",
              "      <td>0.01857</td>\n",
              "      <td>0.005466</td>\n",
              "      <td>17.46</td>\n",
              "      <td>37.13</td>\n",
              "      <td>124.10</td>\n",
              "      <td>943.2</td>\n",
              "      <td>0.1678</td>\n",
              "      <td>0.6577</td>\n",
              "      <td>0.70260</td>\n",
              "      <td>0.17120</td>\n",
              "      <td>0.4218</td>\n",
              "      <td>0.13410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>848406</td>\n",
              "      <td>M</td>\n",
              "      <td>14.680</td>\n",
              "      <td>20.13</td>\n",
              "      <td>94.74</td>\n",
              "      <td>684.5</td>\n",
              "      <td>0.09867</td>\n",
              "      <td>0.07200</td>\n",
              "      <td>0.07395</td>\n",
              "      <td>0.05259</td>\n",
              "      <td>0.1586</td>\n",
              "      <td>0.05922</td>\n",
              "      <td>0.4727</td>\n",
              "      <td>1.2400</td>\n",
              "      <td>3.195</td>\n",
              "      <td>45.40</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>0.011620</td>\n",
              "      <td>0.01998</td>\n",
              "      <td>0.011090</td>\n",
              "      <td>0.01410</td>\n",
              "      <td>0.002085</td>\n",
              "      <td>19.07</td>\n",
              "      <td>30.88</td>\n",
              "      <td>123.40</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>0.1464</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.29140</td>\n",
              "      <td>0.16090</td>\n",
              "      <td>0.3029</td>\n",
              "      <td>0.08216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>84862001</td>\n",
              "      <td>M</td>\n",
              "      <td>16.130</td>\n",
              "      <td>20.68</td>\n",
              "      <td>108.10</td>\n",
              "      <td>798.8</td>\n",
              "      <td>0.11700</td>\n",
              "      <td>0.20220</td>\n",
              "      <td>0.17220</td>\n",
              "      <td>0.10280</td>\n",
              "      <td>0.2164</td>\n",
              "      <td>0.07356</td>\n",
              "      <td>0.5692</td>\n",
              "      <td>1.0730</td>\n",
              "      <td>3.854</td>\n",
              "      <td>54.18</td>\n",
              "      <td>0.007026</td>\n",
              "      <td>0.025010</td>\n",
              "      <td>0.03188</td>\n",
              "      <td>0.012970</td>\n",
              "      <td>0.01689</td>\n",
              "      <td>0.004142</td>\n",
              "      <td>20.96</td>\n",
              "      <td>31.48</td>\n",
              "      <td>136.80</td>\n",
              "      <td>1315.0</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>0.4233</td>\n",
              "      <td>0.47840</td>\n",
              "      <td>0.20730</td>\n",
              "      <td>0.3706</td>\n",
              "      <td>0.11420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>849014</td>\n",
              "      <td>M</td>\n",
              "      <td>19.810</td>\n",
              "      <td>22.15</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1260.0</td>\n",
              "      <td>0.09831</td>\n",
              "      <td>0.10270</td>\n",
              "      <td>0.14790</td>\n",
              "      <td>0.09498</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.05395</td>\n",
              "      <td>0.7582</td>\n",
              "      <td>1.0170</td>\n",
              "      <td>5.865</td>\n",
              "      <td>112.40</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>0.018930</td>\n",
              "      <td>0.03391</td>\n",
              "      <td>0.015210</td>\n",
              "      <td>0.01356</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>27.32</td>\n",
              "      <td>30.88</td>\n",
              "      <td>186.80</td>\n",
              "      <td>2398.0</td>\n",
              "      <td>0.1512</td>\n",
              "      <td>0.3150</td>\n",
              "      <td>0.53720</td>\n",
              "      <td>0.23880</td>\n",
              "      <td>0.2768</td>\n",
              "      <td>0.07615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>8510426</td>\n",
              "      <td>B</td>\n",
              "      <td>13.540</td>\n",
              "      <td>14.36</td>\n",
              "      <td>87.46</td>\n",
              "      <td>566.3</td>\n",
              "      <td>0.09779</td>\n",
              "      <td>0.08129</td>\n",
              "      <td>0.06664</td>\n",
              "      <td>0.04781</td>\n",
              "      <td>0.1885</td>\n",
              "      <td>0.05766</td>\n",
              "      <td>0.2699</td>\n",
              "      <td>0.7886</td>\n",
              "      <td>2.058</td>\n",
              "      <td>23.56</td>\n",
              "      <td>0.008462</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.02387</td>\n",
              "      <td>0.013150</td>\n",
              "      <td>0.01980</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>15.11</td>\n",
              "      <td>19.26</td>\n",
              "      <td>99.70</td>\n",
              "      <td>711.2</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.1773</td>\n",
              "      <td>0.23900</td>\n",
              "      <td>0.12880</td>\n",
              "      <td>0.2977</td>\n",
              "      <td>0.07259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>8510653</td>\n",
              "      <td>B</td>\n",
              "      <td>13.080</td>\n",
              "      <td>15.71</td>\n",
              "      <td>85.63</td>\n",
              "      <td>520.0</td>\n",
              "      <td>0.10750</td>\n",
              "      <td>0.12700</td>\n",
              "      <td>0.04568</td>\n",
              "      <td>0.03110</td>\n",
              "      <td>0.1967</td>\n",
              "      <td>0.06811</td>\n",
              "      <td>0.1852</td>\n",
              "      <td>0.7477</td>\n",
              "      <td>1.383</td>\n",
              "      <td>14.67</td>\n",
              "      <td>0.004097</td>\n",
              "      <td>0.018980</td>\n",
              "      <td>0.01698</td>\n",
              "      <td>0.006490</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.002425</td>\n",
              "      <td>14.50</td>\n",
              "      <td>20.49</td>\n",
              "      <td>96.09</td>\n",
              "      <td>630.5</td>\n",
              "      <td>0.1312</td>\n",
              "      <td>0.2776</td>\n",
              "      <td>0.18900</td>\n",
              "      <td>0.07283</td>\n",
              "      <td>0.3184</td>\n",
              "      <td>0.08183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8510824</td>\n",
              "      <td>B</td>\n",
              "      <td>9.504</td>\n",
              "      <td>12.44</td>\n",
              "      <td>60.34</td>\n",
              "      <td>273.9</td>\n",
              "      <td>0.10240</td>\n",
              "      <td>0.06492</td>\n",
              "      <td>0.02956</td>\n",
              "      <td>0.02076</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.2773</td>\n",
              "      <td>0.9768</td>\n",
              "      <td>1.909</td>\n",
              "      <td>15.70</td>\n",
              "      <td>0.009606</td>\n",
              "      <td>0.014320</td>\n",
              "      <td>0.01985</td>\n",
              "      <td>0.014210</td>\n",
              "      <td>0.02027</td>\n",
              "      <td>0.002968</td>\n",
              "      <td>10.23</td>\n",
              "      <td>15.66</td>\n",
              "      <td>65.13</td>\n",
              "      <td>314.9</td>\n",
              "      <td>0.1324</td>\n",
              "      <td>0.1148</td>\n",
              "      <td>0.08867</td>\n",
              "      <td>0.06227</td>\n",
              "      <td>0.2450</td>\n",
              "      <td>0.07773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8511133</td>\n",
              "      <td>M</td>\n",
              "      <td>15.340</td>\n",
              "      <td>14.26</td>\n",
              "      <td>102.50</td>\n",
              "      <td>704.4</td>\n",
              "      <td>0.10730</td>\n",
              "      <td>0.21350</td>\n",
              "      <td>0.20770</td>\n",
              "      <td>0.09756</td>\n",
              "      <td>0.2521</td>\n",
              "      <td>0.07032</td>\n",
              "      <td>0.4388</td>\n",
              "      <td>0.7096</td>\n",
              "      <td>3.384</td>\n",
              "      <td>44.91</td>\n",
              "      <td>0.006789</td>\n",
              "      <td>0.053280</td>\n",
              "      <td>0.06446</td>\n",
              "      <td>0.022520</td>\n",
              "      <td>0.03672</td>\n",
              "      <td>0.004394</td>\n",
              "      <td>18.07</td>\n",
              "      <td>19.08</td>\n",
              "      <td>125.10</td>\n",
              "      <td>980.9</td>\n",
              "      <td>0.1390</td>\n",
              "      <td>0.5954</td>\n",
              "      <td>0.63050</td>\n",
              "      <td>0.23930</td>\n",
              "      <td>0.4667</td>\n",
              "      <td>0.09946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>851509</td>\n",
              "      <td>M</td>\n",
              "      <td>21.160</td>\n",
              "      <td>23.04</td>\n",
              "      <td>137.20</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>0.09428</td>\n",
              "      <td>0.10220</td>\n",
              "      <td>0.10970</td>\n",
              "      <td>0.08632</td>\n",
              "      <td>0.1769</td>\n",
              "      <td>0.05278</td>\n",
              "      <td>0.6917</td>\n",
              "      <td>1.1270</td>\n",
              "      <td>4.303</td>\n",
              "      <td>93.99</td>\n",
              "      <td>0.004728</td>\n",
              "      <td>0.012590</td>\n",
              "      <td>0.01715</td>\n",
              "      <td>0.010380</td>\n",
              "      <td>0.01083</td>\n",
              "      <td>0.001987</td>\n",
              "      <td>29.17</td>\n",
              "      <td>35.59</td>\n",
              "      <td>188.00</td>\n",
              "      <td>2615.0</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.2600</td>\n",
              "      <td>0.31550</td>\n",
              "      <td>0.20090</td>\n",
              "      <td>0.2822</td>\n",
              "      <td>0.07526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>852552</td>\n",
              "      <td>M</td>\n",
              "      <td>16.650</td>\n",
              "      <td>21.38</td>\n",
              "      <td>110.00</td>\n",
              "      <td>904.6</td>\n",
              "      <td>0.11210</td>\n",
              "      <td>0.14570</td>\n",
              "      <td>0.15250</td>\n",
              "      <td>0.09170</td>\n",
              "      <td>0.1995</td>\n",
              "      <td>0.06330</td>\n",
              "      <td>0.8068</td>\n",
              "      <td>0.9017</td>\n",
              "      <td>5.455</td>\n",
              "      <td>102.60</td>\n",
              "      <td>0.006048</td>\n",
              "      <td>0.018820</td>\n",
              "      <td>0.02741</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.01468</td>\n",
              "      <td>0.002801</td>\n",
              "      <td>26.46</td>\n",
              "      <td>31.56</td>\n",
              "      <td>177.00</td>\n",
              "      <td>2215.0</td>\n",
              "      <td>0.1805</td>\n",
              "      <td>0.3578</td>\n",
              "      <td>0.46950</td>\n",
              "      <td>0.20950</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.09564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>852631</td>\n",
              "      <td>M</td>\n",
              "      <td>17.140</td>\n",
              "      <td>16.40</td>\n",
              "      <td>116.00</td>\n",
              "      <td>912.7</td>\n",
              "      <td>0.11860</td>\n",
              "      <td>0.22760</td>\n",
              "      <td>0.22290</td>\n",
              "      <td>0.14010</td>\n",
              "      <td>0.3040</td>\n",
              "      <td>0.07413</td>\n",
              "      <td>1.0460</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>7.276</td>\n",
              "      <td>111.40</td>\n",
              "      <td>0.008029</td>\n",
              "      <td>0.037990</td>\n",
              "      <td>0.03732</td>\n",
              "      <td>0.023970</td>\n",
              "      <td>0.02308</td>\n",
              "      <td>0.007444</td>\n",
              "      <td>22.25</td>\n",
              "      <td>21.40</td>\n",
              "      <td>152.40</td>\n",
              "      <td>1461.0</td>\n",
              "      <td>0.1545</td>\n",
              "      <td>0.3949</td>\n",
              "      <td>0.38530</td>\n",
              "      <td>0.25500</td>\n",
              "      <td>0.4066</td>\n",
              "      <td>0.10590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>852763</td>\n",
              "      <td>M</td>\n",
              "      <td>14.580</td>\n",
              "      <td>21.53</td>\n",
              "      <td>97.41</td>\n",
              "      <td>644.8</td>\n",
              "      <td>0.10540</td>\n",
              "      <td>0.18680</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.08783</td>\n",
              "      <td>0.2252</td>\n",
              "      <td>0.06924</td>\n",
              "      <td>0.2545</td>\n",
              "      <td>0.9832</td>\n",
              "      <td>2.110</td>\n",
              "      <td>21.05</td>\n",
              "      <td>0.004452</td>\n",
              "      <td>0.030550</td>\n",
              "      <td>0.02681</td>\n",
              "      <td>0.013520</td>\n",
              "      <td>0.01454</td>\n",
              "      <td>0.003711</td>\n",
              "      <td>17.62</td>\n",
              "      <td>33.21</td>\n",
              "      <td>122.40</td>\n",
              "      <td>896.9</td>\n",
              "      <td>0.1525</td>\n",
              "      <td>0.6643</td>\n",
              "      <td>0.55390</td>\n",
              "      <td>0.27010</td>\n",
              "      <td>0.4264</td>\n",
              "      <td>0.12750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>852781</td>\n",
              "      <td>M</td>\n",
              "      <td>18.610</td>\n",
              "      <td>20.25</td>\n",
              "      <td>122.10</td>\n",
              "      <td>1094.0</td>\n",
              "      <td>0.09440</td>\n",
              "      <td>0.10660</td>\n",
              "      <td>0.14900</td>\n",
              "      <td>0.07731</td>\n",
              "      <td>0.1697</td>\n",
              "      <td>0.05699</td>\n",
              "      <td>0.8529</td>\n",
              "      <td>1.8490</td>\n",
              "      <td>5.632</td>\n",
              "      <td>93.54</td>\n",
              "      <td>0.010750</td>\n",
              "      <td>0.027220</td>\n",
              "      <td>0.05081</td>\n",
              "      <td>0.019110</td>\n",
              "      <td>0.02293</td>\n",
              "      <td>0.004217</td>\n",
              "      <td>21.31</td>\n",
              "      <td>27.26</td>\n",
              "      <td>139.90</td>\n",
              "      <td>1403.0</td>\n",
              "      <td>0.1338</td>\n",
              "      <td>0.2117</td>\n",
              "      <td>0.34460</td>\n",
              "      <td>0.14900</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.07421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>852973</td>\n",
              "      <td>M</td>\n",
              "      <td>15.300</td>\n",
              "      <td>25.27</td>\n",
              "      <td>102.40</td>\n",
              "      <td>732.4</td>\n",
              "      <td>0.10820</td>\n",
              "      <td>0.16970</td>\n",
              "      <td>0.16830</td>\n",
              "      <td>0.08751</td>\n",
              "      <td>0.1926</td>\n",
              "      <td>0.06540</td>\n",
              "      <td>0.4390</td>\n",
              "      <td>1.0120</td>\n",
              "      <td>3.498</td>\n",
              "      <td>43.50</td>\n",
              "      <td>0.005233</td>\n",
              "      <td>0.030570</td>\n",
              "      <td>0.03576</td>\n",
              "      <td>0.010830</td>\n",
              "      <td>0.01768</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>20.27</td>\n",
              "      <td>36.71</td>\n",
              "      <td>149.30</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>0.1641</td>\n",
              "      <td>0.6110</td>\n",
              "      <td>0.63350</td>\n",
              "      <td>0.20240</td>\n",
              "      <td>0.4027</td>\n",
              "      <td>0.09876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>853201</td>\n",
              "      <td>M</td>\n",
              "      <td>17.570</td>\n",
              "      <td>15.05</td>\n",
              "      <td>115.00</td>\n",
              "      <td>955.1</td>\n",
              "      <td>0.09847</td>\n",
              "      <td>0.11570</td>\n",
              "      <td>0.09875</td>\n",
              "      <td>0.07953</td>\n",
              "      <td>0.1739</td>\n",
              "      <td>0.06149</td>\n",
              "      <td>0.6003</td>\n",
              "      <td>0.8225</td>\n",
              "      <td>4.655</td>\n",
              "      <td>61.10</td>\n",
              "      <td>0.005627</td>\n",
              "      <td>0.030330</td>\n",
              "      <td>0.03407</td>\n",
              "      <td>0.013540</td>\n",
              "      <td>0.01925</td>\n",
              "      <td>0.003742</td>\n",
              "      <td>20.01</td>\n",
              "      <td>19.52</td>\n",
              "      <td>134.90</td>\n",
              "      <td>1227.0</td>\n",
              "      <td>0.1255</td>\n",
              "      <td>0.2812</td>\n",
              "      <td>0.24890</td>\n",
              "      <td>0.14560</td>\n",
              "      <td>0.2756</td>\n",
              "      <td>0.07919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id diagnosis  attr 1  attr 2  ...  attr 27  attr 28  attr 29  attr 30\n",
              "0     842302         M  17.990   10.38  ...  0.71190  0.26540   0.4601  0.11890\n",
              "1     842517         M  20.570   17.77  ...  0.24160  0.18600   0.2750  0.08902\n",
              "2   84300903         M  19.690   21.25  ...  0.45040  0.24300   0.3613  0.08758\n",
              "3   84348301         M  11.420   20.38  ...  0.68690  0.25750   0.6638  0.17300\n",
              "4   84358402         M  20.290   14.34  ...  0.40000  0.16250   0.2364  0.07678\n",
              "5     843786         M  12.450   15.70  ...  0.53550  0.17410   0.3985  0.12440\n",
              "6     844359         M  18.250   19.98  ...  0.37840  0.19320   0.3063  0.08368\n",
              "7   84458202         M  13.710   20.83  ...  0.26780  0.15560   0.3196  0.11510\n",
              "8     844981         M  13.000   21.82  ...  0.53900  0.20600   0.4378  0.10720\n",
              "9   84501001         M  12.460   24.04  ...  1.10500  0.22100   0.4366  0.20750\n",
              "10    845636         M  16.020   23.24  ...  0.14590  0.09975   0.2948  0.08452\n",
              "11  84610002         M  15.780   17.89  ...  0.39650  0.18100   0.3792  0.10480\n",
              "12    846226         M  19.170   24.80  ...  0.36390  0.17670   0.3176  0.10230\n",
              "13    846381         M  15.850   23.95  ...  0.23220  0.11190   0.2809  0.06287\n",
              "14  84667401         M  13.730   22.61  ...  0.69430  0.22080   0.3596  0.14310\n",
              "15  84799002         M  14.540   27.54  ...  0.70260  0.17120   0.4218  0.13410\n",
              "16    848406         M  14.680   20.13  ...  0.29140  0.16090   0.3029  0.08216\n",
              "17  84862001         M  16.130   20.68  ...  0.47840  0.20730   0.3706  0.11420\n",
              "18    849014         M  19.810   22.15  ...  0.53720  0.23880   0.2768  0.07615\n",
              "19   8510426         B  13.540   14.36  ...  0.23900  0.12880   0.2977  0.07259\n",
              "20   8510653         B  13.080   15.71  ...  0.18900  0.07283   0.3184  0.08183\n",
              "21   8510824         B   9.504   12.44  ...  0.08867  0.06227   0.2450  0.07773\n",
              "22   8511133         M  15.340   14.26  ...  0.63050  0.23930   0.4667  0.09946\n",
              "23    851509         M  21.160   23.04  ...  0.31550  0.20090   0.2822  0.07526\n",
              "24    852552         M  16.650   21.38  ...  0.46950  0.20950   0.3613  0.09564\n",
              "25    852631         M  17.140   16.40  ...  0.38530  0.25500   0.4066  0.10590\n",
              "26    852763         M  14.580   21.53  ...  0.55390  0.27010   0.4264  0.12750\n",
              "27    852781         M  18.610   20.25  ...  0.34460  0.14900   0.2341  0.07421\n",
              "28    852973         M  15.300   25.27  ...  0.63350  0.20240   0.4027  0.09876\n",
              "29    853201         M  17.570   15.05  ...  0.24890  0.14560   0.2756  0.07919\n",
              "\n",
              "[30 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zybgbSzra2f"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wERxZP5nx3No"
      },
      "source": [
        "## 1- create feature and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkIhxCvTo_f0",
        "outputId": "be6a6abc-18f0-4949-9083-0197e23b6ad2"
      },
      "source": [
        "#create feature and label\r\n",
        "Data = pd.DataFrame(data)\r\n",
        "print(Data)\r\n",
        "label = Data.loc[:, 'diagnosis'].replace(['B','M'], [0,1])\r\n",
        "print(label)\r\n",
        "print(label.shape)\r\n",
        "label[22]\r\n",
        "Data = Data.drop(columns=['Id', 'diagnosis'])\r\n",
        "print(Data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Id diagnosis  attr 1  attr 2  ...  attr 27  attr 28  attr 29  attr 30\n",
            "0      842302         M   17.99   10.38  ...   0.7119   0.2654   0.4601  0.11890\n",
            "1      842517         M   20.57   17.77  ...   0.2416   0.1860   0.2750  0.08902\n",
            "2    84300903         M   19.69   21.25  ...   0.4504   0.2430   0.3613  0.08758\n",
            "3    84348301         M   11.42   20.38  ...   0.6869   0.2575   0.6638  0.17300\n",
            "4    84358402         M   20.29   14.34  ...   0.4000   0.1625   0.2364  0.07678\n",
            "..        ...       ...     ...     ...  ...      ...      ...      ...      ...\n",
            "564    926424         M   21.56   22.39  ...   0.4107   0.2216   0.2060  0.07115\n",
            "565    926682         M   20.13   28.25  ...   0.3215   0.1628   0.2572  0.06637\n",
            "566    926954         M   16.60   28.08  ...   0.3403   0.1418   0.2218  0.07820\n",
            "567    927241         M   20.60   29.33  ...   0.9387   0.2650   0.4087  0.12400\n",
            "568     92751         B    7.76   24.54  ...   0.0000   0.0000   0.2871  0.07039\n",
            "\n",
            "[569 rows x 32 columns]\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      1\n",
            "      ..\n",
            "564    1\n",
            "565    1\n",
            "566    1\n",
            "567    1\n",
            "568    0\n",
            "Name: diagnosis, Length: 569, dtype: int64\n",
            "(569,)\n",
            "     attr 1  attr 2  attr 3  attr 4  ...  attr 27  attr 28  attr 29  attr 30\n",
            "0     17.99   10.38  122.80  1001.0  ...   0.7119   0.2654   0.4601  0.11890\n",
            "1     20.57   17.77  132.90  1326.0  ...   0.2416   0.1860   0.2750  0.08902\n",
            "2     19.69   21.25  130.00  1203.0  ...   0.4504   0.2430   0.3613  0.08758\n",
            "3     11.42   20.38   77.58   386.1  ...   0.6869   0.2575   0.6638  0.17300\n",
            "4     20.29   14.34  135.10  1297.0  ...   0.4000   0.1625   0.2364  0.07678\n",
            "..      ...     ...     ...     ...  ...      ...      ...      ...      ...\n",
            "564   21.56   22.39  142.00  1479.0  ...   0.4107   0.2216   0.2060  0.07115\n",
            "565   20.13   28.25  131.20  1261.0  ...   0.3215   0.1628   0.2572  0.06637\n",
            "566   16.60   28.08  108.30   858.1  ...   0.3403   0.1418   0.2218  0.07820\n",
            "567   20.60   29.33  140.10  1265.0  ...   0.9387   0.2650   0.4087  0.12400\n",
            "568    7.76   24.54   47.92   181.0  ...   0.0000   0.0000   0.2871  0.07039\n",
            "\n",
            "[569 rows x 30 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CKNdbDnx7uX"
      },
      "source": [
        "## 2- split to train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37xwKfOTyArq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d972d35-8657-4acc-9bdc-bc61ed00e2d1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(Data, label,test_size=0.3,\r\n",
        "                                                   random_state = 0)\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(398, 30)\n",
            "(171, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "Ixe9ulLcJyeB",
        "outputId": "ac8d51b4-beb6-48bb-e3b0-2955451c97ea"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "\r\n",
        "\r\n",
        "pca1 = PCA().fit(X_train)\r\n",
        "plt.plot(pca1.explained_variance_ratio_)\r\n",
        "plt.xlabel('number of components')\r\n",
        "plt.ylabel('explained variance')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wYkzUvn3OHr"
      },
      "source": [
        "##3- Scaling\r\n",
        "by using Standard Scaler\r\n",
        "It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as:\r\n",
        "\r\n",
        "z = (x - u) / s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ae43gmuWoi",
        "outputId": "cff4dc71-dc31-4a73-a51a-a180a7427f4e"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_test = scaler.transform(X_test)\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(398, 30)\n",
            "(171, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnp2ZbWlGXFl",
        "outputId": "a6bba7b0-a6ae-4d96-a785-81cf25750cfa"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "params = { \r\n",
        "    'C': [1.0, 2.0, 2.5, 5.0, 10, 100, 1000]\r\n",
        "}\r\n",
        "\r\n",
        "model_GSCV = GridSearchCV(LogisticRegression(),scoring='accuracy', param_grid = params, cv = 10)\r\n",
        "\r\n",
        "model_GSCV.fit(X_train, y_train)\r\n",
        "print(\"best_parameters of model\",model_GSCV.best_params_ )\r\n",
        "y_pred = model_GSCV.predict(X_test)\r\n",
        "\r\n",
        "# Model Accuracy\r\n",
        "print(\"Train Accuracy: %.3f\"% model_GSCV.score(X_train, y_train))\r\n",
        "print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\r\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\r\n",
        "print(metrics.classification_report(y_test, y_pred,target_names=['M','B']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best_parameters of model {'C': 5.0}\n",
            "Train Accuracy: 0.952\n",
            "Test Accuracy: 0.9532163742690059\n",
            "[[101   7]\n",
            " [  1  62]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.99      0.94      0.96       108\n",
            "           B       0.90      0.98      0.94        63\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.94      0.96      0.95       171\n",
            "weighted avg       0.96      0.95      0.95       171\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqS-6MC4Tra"
      },
      "source": [
        "##4- feature selection/ feature projection\r\n",
        "Dimentionality reduction using PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMjMoHsC2bWo",
        "outputId": "9e1271b9-e6cf-4b94-9d21-9bbd847213a0"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "\r\n",
        "pca = PCA(0.9)\r\n",
        "X_train_pca = pca.fit_transform(X_train)\r\n",
        "X_test_pca = pca.transform(X_test)\r\n",
        "X_train = pd.DataFrame(data = X_train_pca)\r\n",
        "X_test = pd.DataFrame(data = X_test_pca)\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_test = scaler.transform(X_test)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(398, 7)\n",
            "(171, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltY1RML6Jc58",
        "outputId": "3494c000-0870-4be0-bf7a-9c4c0721bca2"
      },
      "source": [
        "print (pca.explained_variance_ratio_ ) #percentage of variance explained by each of the selected components"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.43689315 0.19415163 0.09661545 0.06716611 0.0549883  0.04012257\n",
            " 0.02183068]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AsutvSn69w5"
      },
      "source": [
        "# a. Classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWmt8GBa5cUs",
        "outputId": "ec79259a-5080-49ab-a32e-708e155fa0fe"
      },
      "source": [
        "#logistic regression model\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "model = LogisticRegression()\r\n",
        "# fit the model with data\r\n",
        "model.fit(X_train, y_train)\r\n",
        "\r\n",
        "y_pred = model.predict(X_test)\r\n",
        "\r\n",
        "# Model Accuracy\r\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\r\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\r\n",
        "print(metrics.classification_report(y_test, y_pred,target_names=['M','B']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "[[107   1]\n",
            " [  5  58]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.96      0.99      0.97       108\n",
            "           B       0.98      0.92      0.95        63\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.96       171\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7rfNqLG8usR"
      },
      "source": [
        "#b. Hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZNrbou-8z9K",
        "outputId": "7cffcd4a-ba3d-4e30-9862-9bab00149877"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "params = { \r\n",
        "    'C': [1.0, 2.0, 2.5, 5.0, 10, 100, 1000]\r\n",
        "}\r\n",
        "\r\n",
        "model_GSCV = GridSearchCV(LogisticRegression(),scoring='accuracy', param_grid = params, cv = 10)\r\n",
        "\r\n",
        "model_GSCV.fit(X_train, y_train)\r\n",
        "print(\"best_parameters of model\",model_GSCV.best_params_ )\r\n",
        "y_pred = model_GSCV.predict(X_test)\r\n",
        "\r\n",
        "# Model Accuracy\r\n",
        "print(\"Train Accuracy: %.3f\"% model_GSCV.score(X_train, y_train))\r\n",
        "print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\r\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\r\n",
        "print(metrics.classification_report(y_test, y_pred,target_names=['M','B']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_parameters of model {'C': 2.5}\n",
            "Train Accuracy: 0.980\n",
            "Test Accuracy: 0.9590643274853801\n",
            "[[106   2]\n",
            " [  5  58]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.95      0.98      0.97       108\n",
            "           B       0.97      0.92      0.94        63\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSxdR656Btvs"
      },
      "source": [
        "    #if {'C': 1.0}\r\n",
        "\r\n",
        "    train score: 0.977\r\n",
        "    Accuracy: 0.9649122807017544\r\n",
        "    [[107   1]\r\n",
        "    [  5  58]]\r\n",
        "              precision    recall  f1-score   support\r\n",
        "\r\n",
        "           M       0.96      0.99      0.97       108\r\n",
        "           B       0.98      0.92      0.95        63\r\n",
        "\r\n",
        "    accuracy                           0.96       171\r\n",
        "    macro avg      0.97      0.96      0.96       171\r\n",
        "    weighted avg   0.97      0.96      0.96       171"
      ]
    }
  ]
}