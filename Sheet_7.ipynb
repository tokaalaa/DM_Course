{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sheet 7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsX3S8/AuNZ1o2LqqNKcMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tokaalaa/DM_Course/blob/main/Sheet_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z6BBnVEQzeO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAtUP8wYQ5PS"
      },
      "source": [
        "def relu(x):\n",
        "    s = np.maximum(0,x)    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfjYTJBMRCU2"
      },
      "source": [
        "def initialize_parameters_random(layers_dims):\n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            # integer representing the number of layers)\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l+1)] = np.random.randn(layers_dims[l], 1) * 0.01\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxdqoLJoStju",
        "outputId": "929b65bf-3ab4-4e06-8b52-ca3b534fb069"
      },
      "source": [
        "parameters = initialize_parameters_random([2,2,1])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b3 = \" + str(parameters[\"b3\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[ 0.01454044 -0.00092839]\n",
            " [ 0.00932367 -0.00113497]]\n",
            "b2 = [[-0.00167012]\n",
            " [ 0.00547458]]\n",
            "W2 = [[0.00944714 0.00865369]]\n",
            "b3 = [[-0.02981323]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq5MUdoAU8zQ"
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "        \n",
        "    # retrieve parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    \n",
        "    # LINEAR -> RELU -> LINEAR -> RELU\n",
        "    z1 = np.dot(W1, X.T) + b2\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(W2, a1) + b3\n",
        "    a2 = relu(z2)\n",
        "\n",
        "    \n",
        "    cache = (z1, a1, W1, b2, z2, a2, W2, b3)\n",
        "    \n",
        "    return a2, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHIr8DOgVgQE"
      },
      "source": [
        "def compute_loss(a2, Y, lamda):\n",
        "    \n",
        "    # retrieve parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    \n",
        "    D = Y.shape[0]\n",
        "    E = (a2 - Y)**2 + lamda * (LA.norm(W2)**2 + LA.norm(W1)**2 + LA.norm(b2)**2 + LA.norm(b3)**2)\n",
        "    loss = 1./D * np.sum(E)\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1zb86DCZAiI"
      },
      "source": [
        "def backward_propagation(X, Y, cache, lamda):\n",
        "  \n",
        "    (z1, a1, W1, b2, z2, a2, W2, b3) = cache\n",
        "\n",
        "    # Top Layer\n",
        "    h = np.where(a2 > 0, 1, 0)\n",
        "   \n",
        "    dW2 = 2 * lamda * W2.reshape(2,1) + 2* (a2 - Y) * h * a1\n",
        "    db3 = 2 * lamda * b3 + 2* (a2 - Y) * h\n",
        "    print(dW2)\n",
        "    print(db3)\n",
        "    # Bottom Layer\n",
        "    dW1 = np.zeros((2,2,4))\n",
        "    for i in range(X.shape[0]):\n",
        "      dW1[:,:,i] = 2 * lamda * W1\n",
        "      if (a2[0,i] > 0):\n",
        "        dW1[:,:,i] += 2 * (a2[0,i] - Y[0,i]) * (W2.reshape(2,1) @ X[i,:].reshape(1,2))\n",
        "\n",
        "    db2 = 2 * lamda * b2 + 2 * (a2 - Y) * h * W2.reshape(2,1)\n",
        "    print(dW1)\n",
        "    print(db2)\n",
        "    gradients = {\"dW2\": dW2, \"db3\": db3,\n",
        "                 \"dW1\": dW1, \"db2\": db2}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT9Zr8XRY8Js"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \n",
        "    L = len(parameters) # number of layers in the neural networks\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    for k in range(L-2):\n",
        "        print(k)\n",
        "        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * np.sum(grads[\"dW\" + str(k+1)],axis = -1)\n",
        "        parameters[\"b\" + str(k+2)] = parameters[\"b\" + str(k+2)] - learning_rate * np.sum(grads[\"db\" + str(k+2)], axis = -1)\n",
        "        print(\"W\" + str(k+1))\n",
        "        print(parameters[\"W\" + str(k+1)].shape)\n",
        "        print(np.sum(grads[\"dW\" + str(k+1)],axis = -1).shape)\n",
        "        print(\"b\" + str(k+2))\n",
        "        print(parameters[\"b\" + str(k+2)].shape)\n",
        "        print(np.sum(grads[\"db\" + str(k+2)], axis = -1).shape)\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdBQyDDNL_Xq"
      },
      "source": [
        "def model(X, Y, learning_rate = 0.1, num_iterations = 10, print_cost = True, lamda = 0.5):\r\n",
        "        \r\n",
        "    grads = {}\r\n",
        "    costs = [] # to keep track of the loss\r\n",
        "    m = X.shape[0] # number of examples\r\n",
        "    layers_dims = [X.shape[1], 2, 1]\r\n",
        "    \r\n",
        "    # Initialize parameters dictionary.\r\n",
        "    parameters = initialize_parameters_random(layers_dims)\r\n",
        "    # Loop (gradient descent)\r\n",
        "\r\n",
        "    for i in range(0, num_iterations):\r\n",
        "\r\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\r\n",
        "        a2, cache = forward_propagation(X, parameters)\r\n",
        "        \r\n",
        "        # Loss\r\n",
        "        cost = compute_loss(a2, Y, lamda)\r\n",
        "\r\n",
        "        # Backward propagation.\r\n",
        "        grads = backward_propagation(X, Y, cache, lamda)\r\n",
        "        print(parameters)\r\n",
        "        # Update parameters.\r\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\r\n",
        "        print(parameters)\r\n",
        "        # Print the loss every 10 iterations\r\n",
        "        if print_cost and i % 10 == 0:\r\n",
        "            print(\"Cost after iteration {}: {}\".format(i, cost))\r\n",
        "            costs.append(cost)\r\n",
        "            \r\n",
        "    # plot the loss\r\n",
        "    plt.plot(costs)\r\n",
        "    plt.ylabel('cost')\r\n",
        "    plt.xlabel('iterations (per hundreds)')\r\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "eYWfKMY8rTln",
        "outputId": "e9c28909-7f4a-473b-e11c-371cf8607415"
      },
      "source": [
        "X = np.array([[0,0],\n",
        "             [0,1],\n",
        "             [1,0],\n",
        "             [1,1]])\n",
        "\n",
        "Y = np.array([[0,1,1,0]])\n",
        "\n",
        "model(X, Y, learning_rate = 0.1, num_iterations = 100, print_cost = True, lamda = 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00365647 -0.03008668  0.00354825  0.00364184]\n",
            " [ 0.00641169  0.00641169  0.00465941  0.00641169]]\n",
            "[[ 0.02223766 -1.97769384 -1.97780276  0.02223069]]\n",
            "[[[-0.0106356  -0.0106356  -0.01767958 -0.01058296]\n",
            "  [ 0.00965296  0.00260935  0.00965296  0.00970559]]\n",
            "\n",
            " [[ 0.0017206   0.0017206  -0.01100786  0.00181572]\n",
            "  [-0.0060176  -0.01874537 -0.0060176  -0.00592248]]]\n",
            "[[ 0.0073435   0.00024723  0.00024685  0.00734347]\n",
            " [-0.00074277 -0.0135657  -0.0135664  -0.00074281]]\n",
            "{'W1': array([[-0.0106356 ,  0.00965296],\n",
            "       [ 0.0017206 , -0.0060176 ]]), 'b2': array([[ 0.00729083],\n",
            "       [-0.00083793]]), 'W2': array([[0.00354825, 0.00641169]]), 'b3': array([[0.00739531]])}\n",
            "0\n",
            "W1\n",
            "(2, 2)\n",
            "(2, 2)\n",
            "b2\n",
            "(2, 2)\n",
            "(2,)\n",
            "1\n",
            "W2\n",
            "(1, 2)\n",
            "(2,)\n",
            "b3\n",
            "(1, 1)\n",
            "(1,)\n",
            "{'W1': array([[-0.00568222,  0.00649087],\n",
            "       [ 0.0022957 , -0.0023473 ]]), 'b2': array([[ 0.00577273,  0.0101526 ],\n",
            "       [-0.00235604,  0.00202384]]), 'W2': array([[0.00547227, 0.00402224]]), 'b3': array([[0.39849813]])}\n",
            "Cost after iteration 0: 1.973280125242332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e2ede3e49c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-bd9259980b92>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X, Y, learning_rate, num_iterations, print_cost, lamda)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6a8e7b178a46>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# LINEAR -> RELU -> LINEAR -> RELU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,4) (2,2) "
          ]
        }
      ]
    }
  ]
}